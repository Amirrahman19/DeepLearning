{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN VS LSTM VS GRU for tweet dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, SimpleRNN, Activation, Dropout, Conv1D\n",
    "from tensorflow.keras.layers import Embedding, Flatten, LSTM, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# https://haochen23.github.io/2020/01/nlp-rnn-sentiment.html#.Y3sIcXZByUk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37244</th>\n",
       "      <td>jesus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37245</th>\n",
       "      <td>kya bhai pure saal chutiya banaya modi aur jab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37246</th>\n",
       "      <td>downvote karna tha par upvote hogaya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37247</th>\n",
       "      <td>haha nice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37248</th>\n",
       "      <td>facebook itself now working bjp’ cell</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37249 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           clean_comment  category\n",
       "0       family mormon have never tried explain them t...         1\n",
       "1      buddhism has very much lot compatible with chr...         1\n",
       "2      seriously don say thing first all they won get...        -1\n",
       "3      what you have learned yours and only yours wha...         0\n",
       "4      for your own benefit you may want read living ...         1\n",
       "...                                                  ...       ...\n",
       "37244                                              jesus         0\n",
       "37245  kya bhai pure saal chutiya banaya modi aur jab...         1\n",
       "37246              downvote karna tha par upvote hogaya          0\n",
       "37247                                         haha nice          1\n",
       "37248             facebook itself now working bjp’ cell          0\n",
       "\n",
       "[37249 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# data = pd.read_csv(\"https://raw.githubusercontent.com/haochen23/nlp-rnn-lstm-sentiment/master/training.1600000.processed.noemoticon.csv\", header=None, encoding='cp437')\n",
    "# print(\"The shape of the original dataset is {}\".format(data.shape))\n",
    "# data\n",
    "\n",
    "raw = pd.read_csv('data/Reddit_Data.csv')\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    15830\n",
       " 0    13142\n",
       "-1     8277\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28732</th>\n",
       "      <td>pakistan seems have closed their airspace agai...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19562</th>\n",
       "      <td>please read and make video about essar tapes e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36608</th>\n",
       "      <td>needed this stand against pakistan atleast yr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15616</th>\n",
       "      <td>anything get little upset when boyfriend play...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14096</th>\n",
       "      <td>from india its great see president trump host...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>what that cube thing they looking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25093</th>\n",
       "      <td>much was looking forward the modi govt come i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4473</th>\n",
       "      <td>not know real joke anymore fuck you april fuc...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13206</th>\n",
       "      <td>points upvoted votes the army here</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35412</th>\n",
       "      <td>yasss yasssssss</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24831 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           clean_comment  category\n",
       "28732  pakistan seems have closed their airspace agai...        -1\n",
       "19562  please read and make video about essar tapes e...         0\n",
       "36608   needed this stand against pakistan atleast yr...         1\n",
       "15616   anything get little upset when boyfriend play...        -1\n",
       "14096   from india its great see president trump host...         1\n",
       "...                                                  ...       ...\n",
       "5485                  what that cube thing they looking          0\n",
       "25093   much was looking forward the modi govt come i...         0\n",
       "4473    not know real joke anymore fuck you april fuc...        -1\n",
       "13206                 points upvoted votes the army here         0\n",
       "35412                                    yasss yasssssss         0\n",
       "\n",
       "[24831 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM = 8277\n",
    "\n",
    "positive = raw[raw['category']==1].sample(8277)\n",
    "neutral = raw[raw['category']==0].sample(8277)\n",
    "negative = raw[raw['category']==-1].sample(8277)\n",
    "\n",
    "data = pd.concat([positive, neutral, negative], axis=0).sample(frac=1)\n",
    "# positive.shape, neutral.shape, negative.shape\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(glove_file, encoding='iso-8859-1'):\n",
    "    print(\"[INFO]Loading GloVe Model...\")\n",
    "    model = {}\n",
    "    with open(glove_file, 'r', encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embeddings = [float(val) for val in split_line[1:]]\n",
    "            model[word] = embeddings\n",
    "    print(\"[INFO] Done...{} words loaded!\".format(len(model)))\n",
    "    return model\n",
    "# adopted from utils.py\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    '''\n",
    "    function to remove stopwords\n",
    "        input: sentence - string of sentence\n",
    "    '''\n",
    "    new = []\n",
    "    # tokenize sentence\n",
    "    sentence = nlp(sentence)\n",
    "    for tk in sentence:\n",
    "        if (tk.is_stop == False) & (tk.pos_ !=\"PUNCT\"):\n",
    "            new.append(tk.string.strip())\n",
    "    # convert back to sentence string\n",
    "    c = \" \".join(str(x) for x in new)\n",
    "    return c\n",
    "\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    '''\n",
    "    function to do lemmatization\n",
    "        input: sentence - string of sentence\n",
    "    '''\n",
    "    sentence = nlp(sentence)\n",
    "    s = \"\"\n",
    "    for w in sentence:\n",
    "        s +=\" \"+w.lemma_\n",
    "    return nlp(s)\n",
    "\n",
    "def sent_vectorizer(sent, model):\n",
    "    '''\n",
    "    sentence vectorizer using the pretrained glove model\n",
    "    '''\n",
    "    sent_vector = np.zeros(200)\n",
    "    num_w = 0\n",
    "    for w in sent.split():\n",
    "        try:\n",
    "            # add up all token vectors to a sent_vector\n",
    "            sent_vector = np.add(sent_vector, model[str(w)])\n",
    "            num_w += 1\n",
    "        except:\n",
    "            pass\n",
    "    return sent_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24831,), (24831, 3))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X = data['clean_comment'].to_numpy().astype('str')\n",
    "data_y = data['category']\n",
    "data_y = pd.get_dummies(data_y).to_numpy()\n",
    "\n",
    "data_X.shape, data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Loading GloVe Model...\n",
      "[INFO] Done...1193514 words loaded!\n"
     ]
    }
   ],
   "source": [
    "# load the glove model\n",
    "glove_model = load_glove_model(\"glove.twitter.27B.200d.txt\", encoding='utf-8')\n",
    "# number of vocab to keep\n",
    "max_vocab = 18000\n",
    "# length of sequence that will generate\n",
    "max_len = 15\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44112 unique tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24831, 15)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(data_X)\n",
    "sequences = tokenizer.texts_to_sequences(data_X)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data_keras = pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "data_keras.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17381, 15), (7450, 15), (17381, 3), (7450, 3))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_keras, data_y, test_size = 0.3, random_state=42)\n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44113\n",
      "Null word embeddings: 12999\n"
     ]
    }
   ],
   "source": [
    "# calculate number of words\n",
    "nb_words = len(tokenizer.word_index) + 1\n",
    "print(f\"Number of words: {nb_words}\")\n",
    "\n",
    "# obtain the word embedding matrix\n",
    "embedding_matrix = np.zeros((nb_words, 200))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove_model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adopted from sent_tran_eval.py\n",
    "def build_model(nb_words, rnn_model=\"SimpleRNN\", embedding_matrix=None):\n",
    "    '''\n",
    "    build_model function:\n",
    "    inputs: \n",
    "        rnn_model - which type of RNN layer to use, choose in (SimpleRNN, LSTM, GRU)\n",
    "        embedding_matrix - whether to use pretrained embeddings or not\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    # add an embedding layer\n",
    "    if embedding_matrix is not None:\n",
    "        model.add(Embedding(nb_words, \n",
    "                        200, \n",
    "                        weights=[embedding_matrix], \n",
    "                        input_length= max_len,\n",
    "                        trainable = False))\n",
    "    else:\n",
    "        model.add(Embedding(nb_words, \n",
    "                        200, \n",
    "                        input_length= max_len,\n",
    "                        trainable = False))\n",
    "        \n",
    "    # add an RNN layer according to rnn_model\n",
    "    if rnn_model == \"SimpleRNN\":\n",
    "        model.add(SimpleRNN(256))\n",
    "    elif rnn_model == \"LSTM\":\n",
    "        model.add(LSTM(256))\n",
    "    else:\n",
    "        model.add(GRU(256))\n",
    "        \n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "145/145 [==============================] - 7s 41ms/step - loss: 0.8824 - accuracy: 0.5720 - val_loss: 0.8002 - val_accuracy: 0.6290\n",
      "Epoch 2/20\n",
      "145/145 [==============================] - 6s 38ms/step - loss: 0.7578 - accuracy: 0.6550 - val_loss: 0.7850 - val_accuracy: 0.6393\n",
      "Epoch 3/20\n",
      "145/145 [==============================] - 6s 41ms/step - loss: 0.6964 - accuracy: 0.6927 - val_loss: 0.8097 - val_accuracy: 0.6373\n",
      "Epoch 4/20\n",
      "145/145 [==============================] - 7s 46ms/step - loss: 0.6181 - accuracy: 0.7379 - val_loss: 0.8208 - val_accuracy: 0.6302\n",
      "Epoch 5/20\n",
      "145/145 [==============================] - 7s 45ms/step - loss: 0.5365 - accuracy: 0.7797 - val_loss: 0.8903 - val_accuracy: 0.6180\n",
      "Epoch 6/20\n",
      "145/145 [==============================] - 7s 45ms/step - loss: 0.4533 - accuracy: 0.8214 - val_loss: 0.9516 - val_accuracy: 0.6314\n",
      "Epoch 7/20\n",
      "145/145 [==============================] - 6s 45ms/step - loss: 0.3525 - accuracy: 0.8636 - val_loss: 1.0838 - val_accuracy: 0.6183\n",
      "Epoch 8/20\n",
      "145/145 [==============================] - 6s 41ms/step - loss: 0.2870 - accuracy: 0.8941 - val_loss: 1.1901 - val_accuracy: 0.6162\n",
      "Epoch 9/20\n",
      "145/145 [==============================] - 6s 38ms/step - loss: 0.2488 - accuracy: 0.9063 - val_loss: 1.2648 - val_accuracy: 0.6129\n",
      "Epoch 10/20\n",
      "145/145 [==============================] - 5s 37ms/step - loss: 0.2208 - accuracy: 0.9187 - val_loss: 1.3280 - val_accuracy: 0.6158\n",
      "Epoch 11/20\n",
      "145/145 [==============================] - 6s 39ms/step - loss: 0.1842 - accuracy: 0.9319 - val_loss: 1.4456 - val_accuracy: 0.6036\n",
      "Epoch 12/20\n",
      "145/145 [==============================] - 6s 39ms/step - loss: 0.1633 - accuracy: 0.9391 - val_loss: 1.5910 - val_accuracy: 0.6075\n",
      "Epoch 13/20\n",
      "145/145 [==============================] - 5s 38ms/step - loss: 0.1329 - accuracy: 0.9508 - val_loss: 1.7046 - val_accuracy: 0.6118\n",
      "Epoch 14/20\n",
      "145/145 [==============================] - 5s 37ms/step - loss: 0.1321 - accuracy: 0.9516 - val_loss: 1.8359 - val_accuracy: 0.6095\n",
      "Epoch 15/20\n",
      "145/145 [==============================] - 6s 38ms/step - loss: 0.1292 - accuracy: 0.9517 - val_loss: 1.6651 - val_accuracy: 0.6185\n",
      "Epoch 16/20\n",
      "145/145 [==============================] - 5s 37ms/step - loss: 0.1078 - accuracy: 0.9611 - val_loss: 1.9224 - val_accuracy: 0.6193\n",
      "Epoch 17/20\n",
      "145/145 [==============================] - 6s 40ms/step - loss: 0.1041 - accuracy: 0.9606 - val_loss: 1.8296 - val_accuracy: 0.6123\n",
      "Epoch 18/20\n",
      "145/145 [==============================] - 6s 38ms/step - loss: 0.1050 - accuracy: 0.9616 - val_loss: 1.8305 - val_accuracy: 0.6111\n",
      "Epoch 19/20\n",
      "145/145 [==============================] - 5s 38ms/step - loss: 0.1025 - accuracy: 0.9634 - val_loss: 2.0025 - val_accuracy: 0.6132\n",
      "Epoch 20/20\n",
      "145/145 [==============================] - 5s 37ms/step - loss: 0.0854 - accuracy: 0.9683 - val_loss: 2.1561 - val_accuracy: 0.6056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b85d572e20>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn = build_model(nb_words, \"SimpleRNN\", embedding_matrix)\n",
    "mode_rnn_history = model_rnn.fit(x_train, y_train, epochs=20, batch_size=120,\n",
    "          validation_data=(x_test, y_test))\n",
    "# predictions = model_rnn.predict(x_test)\n",
    "# predictions = predictions.argmax(axis=1)\n",
    "# print(classification_report(y_test.argmax(axis=1), predictions))\n",
    "#, callbacks=EarlyStopping(monitor='val_accuracy', mode='max',patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 2s 9ms/step - loss: 2.1561 - accuracy: 0.6056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.15606951713562, 0.6056376099586487]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val_acc_epoch = np.argmax(list(mode_rnn_history.history['val_accuracy']))+1\n",
    "max_val_loss_epoch = np.argmin(list(mode_rnn_history.history['val_loss']))+1\n",
    "epochs = range(1, len(mode_rnn_history.history['accuracy']) + 1)\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.minorticks_on()\n",
    "plt.axvline(x=max_val_acc_epoch, color='0.5', linestyle='--')\n",
    "plt.axvline(x=max_val_loss_epoch, color='0.5', linestyle=':')\n",
    "plt.plot(epochs,mode_rnn_history.history['accuracy'], label='Train')\n",
    "plt.plot(epochs,mode_rnn_history.history['val_accuracy'], label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.minorticks_on()\n",
    "plt.axvline(x=max_val_acc_epoch, color='0.5', linestyle='--')\n",
    "plt.axvline(x=max_val_loss_epoch, color='0.5', linestyle=':')\n",
    "plt.plot(epochs,mode_rnn_history.history['loss'], label='Train')\n",
    "plt.plot(epochs,mode_rnn_history.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "visualkeras.layered_view(model_rnn, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.save_weights(\"models/rnn-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "145/145 [==============================] - 4s 15ms/step - loss: 0.8232 - accuracy: 0.6110 - val_loss: 0.7347 - val_accuracy: 0.6738\n",
      "Epoch 2/20\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.6948 - accuracy: 0.6891 - val_loss: 0.6977 - val_accuracy: 0.6906\n",
      "Epoch 3/20\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.6250 - accuracy: 0.7211 - val_loss: 0.6538 - val_accuracy: 0.7154\n",
      "Epoch 4/20\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.5580 - accuracy: 0.7566 - val_loss: 0.6371 - val_accuracy: 0.7220\n",
      "Epoch 5/20\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4855 - accuracy: 0.7920 - val_loss: 0.6499 - val_accuracy: 0.7432\n",
      "Epoch 6/20\n",
      "145/145 [==============================] - 2s 15ms/step - loss: 0.4141 - accuracy: 0.8265 - val_loss: 0.6438 - val_accuracy: 0.7396\n",
      "Epoch 7/20\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.3355 - accuracy: 0.8661 - val_loss: 0.7199 - val_accuracy: 0.7400\n",
      "Epoch 8/20\n",
      "145/145 [==============================] - 2s 15ms/step - loss: 0.2513 - accuracy: 0.9025 - val_loss: 0.8367 - val_accuracy: 0.7395\n",
      "Epoch 9/20\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.1867 - accuracy: 0.9299 - val_loss: 0.9647 - val_accuracy: 0.7349\n",
      "Epoch 10/20\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.1318 - accuracy: 0.9524 - val_loss: 1.1308 - val_accuracy: 0.7337\n",
      "Epoch 11/20\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.0977 - accuracy: 0.9645 - val_loss: 1.2405 - val_accuracy: 0.7372\n",
      "Epoch 12/20\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.0742 - accuracy: 0.9740 - val_loss: 1.4133 - val_accuracy: 0.7415\n",
      "Epoch 13/20\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.0550 - accuracy: 0.9815 - val_loss: 1.5974 - val_accuracy: 0.7467\n",
      "Epoch 14/20\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.0545 - accuracy: 0.9804 - val_loss: 1.5234 - val_accuracy: 0.7428\n",
      "Epoch 15/20\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.0358 - accuracy: 0.9875 - val_loss: 1.6326 - val_accuracy: 0.7413\n",
      "Epoch 16/20\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.0447 - accuracy: 0.9851 - val_loss: 1.6741 - val_accuracy: 0.7426\n",
      "Epoch 17/20\n",
      "145/145 [==============================] - 2s 16ms/step - loss: 0.0286 - accuracy: 0.9906 - val_loss: 1.7675 - val_accuracy: 0.7396\n",
      "Epoch 18/20\n",
      "145/145 [==============================] - 2s 15ms/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 1.8260 - val_accuracy: 0.7431\n",
      "Epoch 19/20\n",
      "145/145 [==============================] - 2s 15ms/step - loss: 0.0295 - accuracy: 0.9907 - val_loss: 1.7392 - val_accuracy: 0.7439\n",
      "Epoch 20/20\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.0227 - accuracy: 0.9917 - val_loss: 1.8574 - val_accuracy: 0.7428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b9f4e277f0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm = build_model(nb_words, \"LSTM\", embedding_matrix)\n",
    "model_lstm_history = model_lstm.fit(x_train, y_train, epochs=20, batch_size=120,\n",
    "          validation_data=(x_test, y_test))\n",
    "# predictions = model_lstm.predict(x_test)\n",
    "# predictions = predictions.argmax(axis=1)\n",
    "# print(classification_report(y_test.argmax(axis=1), predictions))\n",
    "#, callbacks=EarlyStopping(monitor='val_accuracy', mode='max',patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 1s 6ms/step - loss: 1.8574 - accuracy: 0.7428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8574057817459106, 0.7428187727928162]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val_acc_epoch = np.argmax(list(model_lstm_history.history['val_accuracy']))+1\n",
    "max_val_loss_epoch = np.argmin(list(model_lstm_history.history['val_loss']))+1\n",
    "epochs = range(1, len(model_lstm_history.history['accuracy']) + 1)\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.minorticks_on()\n",
    "plt.axvline(x=max_val_acc_epoch, color='0.5', linestyle='--')\n",
    "plt.axvline(x=max_val_loss_epoch, color='0.5', linestyle=':')\n",
    "plt.plot(epochs,model_lstm_history.history['accuracy'], label='Train')\n",
    "plt.plot(epochs,model_lstm_history.history['val_accuracy'], label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.minorticks_on()\n",
    "plt.axvline(x=max_val_acc_epoch, color='0.5', linestyle='--')\n",
    "plt.axvline(x=max_val_loss_epoch, color='0.5', linestyle=':')\n",
    "plt.plot(epochs,model_lstm_history.history['loss'], label='Train')\n",
    "plt.plot(epochs,model_lstm_history.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "visualkeras.layered_view(model_lstm, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.save_weights(\"models/model-lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "145/145 [==============================] - 7s 14ms/step - loss: 0.8440 - accuracy: 0.5978 - val_loss: 0.7141 - val_accuracy: 0.6827\n",
      "Epoch 2/20\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.6553 - accuracy: 0.7125 - val_loss: 0.6315 - val_accuracy: 0.7260\n",
      "Epoch 3/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.5762 - accuracy: 0.7536 - val_loss: 0.6151 - val_accuracy: 0.7395\n",
      "Epoch 4/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.5148 - accuracy: 0.7818 - val_loss: 0.5968 - val_accuracy: 0.7468\n",
      "Epoch 5/20\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4420 - accuracy: 0.8170 - val_loss: 0.6024 - val_accuracy: 0.7538\n",
      "Epoch 6/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.3669 - accuracy: 0.8531 - val_loss: 0.6247 - val_accuracy: 0.7544\n",
      "Epoch 7/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.2968 - accuracy: 0.8852 - val_loss: 0.7439 - val_accuracy: 0.7528\n",
      "Epoch 8/20\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.2157 - accuracy: 0.9176 - val_loss: 0.8215 - val_accuracy: 0.7605\n",
      "Epoch 9/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.1420 - accuracy: 0.9499 - val_loss: 0.9601 - val_accuracy: 0.7518\n",
      "Epoch 10/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.1001 - accuracy: 0.9657 - val_loss: 1.1461 - val_accuracy: 0.7404\n",
      "Epoch 11/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.0783 - accuracy: 0.9732 - val_loss: 1.2185 - val_accuracy: 0.7397\n",
      "Epoch 12/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.0509 - accuracy: 0.9835 - val_loss: 1.3705 - val_accuracy: 0.7515\n",
      "Epoch 13/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.0361 - accuracy: 0.9879 - val_loss: 1.4863 - val_accuracy: 0.7509\n",
      "Epoch 14/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.0321 - accuracy: 0.9892 - val_loss: 1.6088 - val_accuracy: 0.7458\n",
      "Epoch 15/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.0396 - accuracy: 0.9864 - val_loss: 1.5279 - val_accuracy: 0.7483\n",
      "Epoch 16/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.0304 - accuracy: 0.9896 - val_loss: 1.6974 - val_accuracy: 0.7538\n",
      "Epoch 17/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.0327 - accuracy: 0.9893 - val_loss: 1.5863 - val_accuracy: 0.7588\n",
      "Epoch 18/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.0171 - accuracy: 0.9944 - val_loss: 1.8572 - val_accuracy: 0.7556\n",
      "Epoch 19/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.0228 - accuracy: 0.9917 - val_loss: 1.6423 - val_accuracy: 0.7426\n",
      "Epoch 20/20\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.0315 - accuracy: 0.9905 - val_loss: 1.6072 - val_accuracy: 0.7499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b635dd8040>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru = build_model(nb_words, \"GRU\", embedding_matrix)\n",
    "model_gru_history = model_gru.fit(x_train, y_train, epochs=20, batch_size=120,\n",
    "          validation_data=(x_test, y_test))\n",
    "# predictions = model_gru.predict(x_test)\n",
    "# predictions = predictions.argmax(axis=1)\n",
    "# print(classification_report(y_test.argmax(axis=1), predictions))\n",
    "#, callbacks=EarlyStopping(monitor='val_accuracy', mode='max',patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 1s 4ms/step - loss: 1.6072 - accuracy: 0.7499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6071900129318237, 0.7499328851699829]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val_acc_epoch = np.argmax(list(model_gru_history.history['val_accuracy']))+1\n",
    "max_val_loss_epoch = np.argmin(list(model_gru_history.history['val_loss']))+1\n",
    "epochs = range(1, len(model_gru_history.history['accuracy']) + 1)\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.minorticks_on()\n",
    "plt.axvline(x=max_val_acc_epoch, color='0.5', linestyle='--')\n",
    "plt.axvline(x=max_val_loss_epoch, color='0.5', linestyle=':')\n",
    "plt.plot(epochs,model_gru_history.history['accuracy'], label='Train')\n",
    "plt.plot(epochs,model_gru_history.history['val_accuracy'], label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.minorticks_on()\n",
    "plt.axvline(x=max_val_acc_epoch, color='0.5', linestyle='--')\n",
    "plt.axvline(x=max_val_loss_epoch, color='0.5', linestyle=':')\n",
    "plt.plot(epochs,model_gru_history.history['loss'], label='Train')\n",
    "plt.plot(epochs,model_gru_history.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "visualkeras.layered_view(model_gru, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.save_weights(\"models/model-gru.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a non deeplearning model (TFIDF vectorization) and other traditional machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28732</th>\n",
       "      <td>pakistan seems have closed their airspace agai...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19562</th>\n",
       "      <td>please read and make video about essar tapes e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36608</th>\n",
       "      <td>needed this stand against pakistan atleast yr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15616</th>\n",
       "      <td>anything get little upset when boyfriend play...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14096</th>\n",
       "      <td>from india its great see president trump host...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>what that cube thing they looking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25093</th>\n",
       "      <td>much was looking forward the modi govt come i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4473</th>\n",
       "      <td>not know real joke anymore fuck you april fuc...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13206</th>\n",
       "      <td>points upvoted votes the army here</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35412</th>\n",
       "      <td>yasss yasssssss</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24831 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           clean_comment  category\n",
       "28732  pakistan seems have closed their airspace agai...        -1\n",
       "19562  please read and make video about essar tapes e...         0\n",
       "36608   needed this stand against pakistan atleast yr...         1\n",
       "15616   anything get little upset when boyfriend play...        -1\n",
       "14096   from india its great see president trump host...         1\n",
       "...                                                  ...       ...\n",
       "5485                  what that cube thing they looking          0\n",
       "25093   much was looking forward the modi govt come i...         0\n",
       "4473    not know real joke anymore fuck you april fuc...        -1\n",
       "13206                 points upvoted votes the army here         0\n",
       "35412                                    yasss yasssssss         0\n",
       "\n",
       "[24831 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = data['clean_comment'].astype('str')\n",
    "y = data['category']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19864, 10000), (4967, 10000))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "x_train_vec = vectorizer.fit_transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "x_train_vec.shape, x_test_vec.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7608214213811154"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train_vec, y_train)\n",
    "model.score(x_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amir_\\anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8361183813166901"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_vec, y_train)\n",
    "model.score(x_test_vec, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fa8ed0ec8714c09cd91893ccb0a16ce555f8377d2d150fb2d8f0fcf554de2cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
